{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "74e492b8",
      "metadata": {
        "id": "74e492b8"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganji759/Flood-Prediction-Using-Machine-Learning/blob/main/Hackathon/Image_Segmentation_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3502f9b9",
      "metadata": {
        "id": "3502f9b9"
      },
      "source": [
        "# **Hackathon IndabaX DRC - UPN**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa8f8331",
      "metadata": {
        "id": "aa8f8331"
      },
      "source": [
        "In this challenge you will code in missing cells. The main goal is to end up training your machine learning model and evaluate it.\n",
        "\n",
        "The winner will be based:\n",
        "- Code complexity\n",
        "- Features\n",
        "- Model consistency\n",
        "\n",
        "## Requierements :\n",
        "- Use the pytorch library for coding\n",
        "- Use"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "916c337a",
      "metadata": {
        "id": "916c337a"
      },
      "source": [
        "Import torch, the neural network, the dataset, dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8ecfef96",
      "metadata": {
        "id": "8ecfef96"
      },
      "outputs": [],
      "source": [
        "# Write in this code cell\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e5f4d91d",
      "metadata": {
        "id": "e5f4d91d"
      },
      "outputs": [],
      "source": [
        "# Other libraries\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import jaccard_score\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f6994e37",
      "metadata": {
        "id": "f6994e37"
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6176b590",
      "metadata": {
        "id": "6176b590"
      },
      "source": [
        "Verify the GPU availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "aab5f71d",
      "metadata": {
        "id": "aab5f71d",
        "outputId": "bb2b5674-308a-4290-c67e-f5b097d9eaa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f0cfbce8",
      "metadata": {
        "id": "f0cfbce8"
      },
      "outputs": [],
      "source": [
        "import os, json\n",
        "os.makedirs(os.path.join(os.path.expanduser(\"~\"), \".kaggle\"), exist_ok=True)\n",
        "with open(os.path.join(os.path.expanduser(\"~\"), \".kaggle\", \"kaggle.json\"), \"w\") as f:\n",
        "    json.dump({\"username\":\"user name\",\"key\":\"api key\"}, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2387d6cd",
      "metadata": {
        "id": "2387d6cd"
      },
      "source": [
        "Create your .kaggle folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "475d9fda",
      "metadata": {
        "id": "475d9fda",
        "outputId": "15cf434c-11b3-4764-8b9a-84c90f2eb5ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The syntax of the command is incorrect.\n"
          ]
        }
      ],
      "source": [
        "!mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2387d13a",
      "metadata": {
        "id": "2387d13a",
        "outputId": "c2744331-68e4-4749-ff3c-1b4b7b2be676",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: --: invalid option\n",
            "Usage:\t/bin/bash [GNU long option] [option] ...\n",
            "\t/bin/bash [GNU long option] [option] script-file ...\n",
            "GNU long options:\n",
            "\t--debug\n",
            "\t--debugger\n",
            "\t--dump-po-strings\n",
            "\t--dump-strings\n",
            "\t--help\n",
            "\t--init-file\n",
            "\t--login\n",
            "\t--noediting\n",
            "\t--noprofile\n",
            "\t--norc\n",
            "\t--posix\n",
            "\t--pretty-print\n",
            "\t--rcfile\n",
            "\t--restricted\n",
            "\t--verbose\n",
            "\t--version\n",
            "Shell options:\n",
            "\t-ilrsD or -c command or -O shopt_option\t\t(invocation only)\n",
            "\t-abefhkmnptuvxBCHP or -o option\n"
          ]
        }
      ],
      "source": [
        "!--chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d8be2075",
      "metadata": {
        "id": "d8be2075",
        "outputId": "9865b026-1d69-4c70-c3b9-8c63c4754a9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\n"
          ]
        }
      ],
      "source": [
        "!cd ~/.kaggle/ && ls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9340278",
      "metadata": {
        "id": "d9340278"
      },
      "source": [
        "Set your Kaggle API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d5ae87c",
      "metadata": {
        "id": "2d5ae87c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"user name\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"api key\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c666733",
      "metadata": {
        "id": "4c666733"
      },
      "source": [
        "Look at the cityscapes dataset and upload the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5be8e3e2",
      "metadata": {
        "id": "5be8e3e2",
        "outputId": "b2a1dd7e-7a27-4e46-a441-53fed0e9ae7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "ref                                                  title                                           size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "---------------------------------------------------  ---------------------------------------  -----------  --------------------------  -------------  ---------  ---------------  \n",
            "shuvoalok/cityscapes                                 cityscapes dataset                         209001313  2023-09-13 21:03:34.607000           6008         25  0.875            \n",
            "balraj98/cityscapes-pix2pix-dataset                  Cityscapes Pix2Pix Dataset                 105595335  2020-10-18 08:10:17.600000            767         17  0.9411765        \n",
            "vikramtiwari/pix2pix-dataset                         pix2pix dataset                           2574957257  2018-07-04 05:54:59.713000           8827        114  0.625            \n",
            "dansbecker/cityscapes-image-pairs                    Cityscapes Image Pairs                     211492512  2018-04-20 13:55:20.627000          18486        264  0.75             \n",
            "yessicatuteja/foggy-cityscapes-image-dataset         Foggy Cityscapes Images                   3171746124  2024-02-18 09:27:16.383000           1330         36  1.0              \n",
            "sakshaymahna/cityscapes-depth-and-segmentation       CityScapes - Depth and Segmentation        673063205  2021-12-04 09:08:13.817000           2368         26  0.625            \n",
            "preetviradiya/cityscapes-dataset-for-gans            Cityscapes dataset for GANs                280287193  2022-02-15 05:10:46.123000            196          9  0.375            \n",
            "ishansrivastava1308/image-map-cityscapes-19510       CityScapes Dataset                       11848210160  2024-06-08 03:29:53.660000             10          1  0.4375           \n",
            "kareemmetwaly/cityscapes-attributes-recognition-car  Cityscapes Attributes Recognition (CAR)        23911  2025-12-07 10:45:55.370000             83         19  0.6875           \n",
            "norod78/sketch2pokemon                               Sketch2Pokemon                             137519017  2019-10-28 09:09:41.103000            490         10  0.8125           \n",
            "sakshaymahna/semantic-segmentation-bev               Semantic Segmentation - BEV               2155380825  2025-12-11 15:54:41.137000            479         22  0.875            \n",
            "hungngu1999/cityscapes-dataset                       Cityscapes dataset                       11848107556  2022-02-17 12:46:12.643000             61          2  0.125            \n",
            "zhangyunsheng/cityscapes-data                        cityscapes_data                            105579467  2020-06-18 07:11:17.440000            937          9  0.6875           \n",
            "utkarshsaxenadn/cityscapessegmentationpix2pixgan     Cityscapes-Segmentation-Pix2PixGAN         201741577  2022-09-13 10:05:09.577000           1271          9  0.9375           \n",
            "zeadomar/geoeye-70-earth-observation                 GEOEYE-70 | Earth Observation             1277327461  2024-04-04 12:07:56.150000             81          7  0.625            \n",
            "peopledream/cityscapes-dataset                       Cityscapes dataset                         856629887  2024-01-20 12:36:20.320000             17          0  0.375            \n",
            "shaniasalsabillaq/cityscapes-dataset                 Cityscapes Dataset                         125672299  2024-07-23 14:39:14.880000              1          0  0.375            \n",
            "manjotpahwa/indian-driving-dataset                   Indian Driving Dataset                    5920724657  2019-11-17 13:37:55.133000           1094         14  0.4375           \n",
            "felixjfj/cityscapes-augmented                        cityscapes_augmented                     44391420830  2025-01-15 17:22:17.203000              9          1  0.6875           \n",
            "beyondstellaris/bluegreen-algae-dataset              Blue-green algae dataset                  2468240161  2022-10-15 07:23:42.603000            275          7  0.375            \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets list -s \"cityscapes dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7a1b95b",
      "metadata": {
        "id": "c7a1b95b"
      },
      "source": [
        "Add that dataset to the path for dataset uploading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "14a89ab9",
      "metadata": {
        "id": "14a89ab9",
        "outputId": "919ed750-fbf0-428d-f176-bc0760adfd12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/shuvoalok/cityscapes?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 199M/199M [00:01<00:00, 159MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/shuvoalok/cityscapes/versions/2\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"shuvoalok/cityscapes\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "ee92d0c6",
      "metadata": {
        "id": "ee92d0c6"
      },
      "outputs": [],
      "source": [
        "# Define paths\n",
        "train_images_folder_path = \"/kaggle/input/cityscapes/train/img\"\n",
        "train_mask_folder_path = \"/kaggle/input/cityscapes/train/label\"\n",
        "test_images_folder_path = \"/kaggle/input/cityscapes/val/img\"\n",
        "test_mask_folder_path = \"/kaggle/input/cityscapes/val/label\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7e51719a",
      "metadata": {
        "id": "7e51719a"
      },
      "outputs": [],
      "source": [
        "# ========== CONSTANTS ==========\n",
        "names = ['unlabeled', 'dynamic', 'ground', 'road', 'sidewalk', 'parking', 'rail track', 'building', 'wall',\n",
        "         'fence', 'guard rail', 'bridge', 'tunnel', 'pole', 'traffic light', 'traffic sign', 'vegetation',\n",
        "         'terrain', 'sky', 'person', 'rider', 'car', 'truck', 'bus', 'caravan', 'trailer', 'train',\n",
        "         'motorcycle', 'bicycle', 'license plate']\n",
        "\n",
        "colors = np.array([\n",
        "    (0, 0, 0), (111, 74, 0), (81, 0, 81), (128, 64, 128), (244, 35, 232),\n",
        "    (250, 170, 160), (230, 150, 140), (70, 70, 70), (102, 102, 156),\n",
        "    (190, 153, 153), (180, 165, 180), (150, 100, 100), (150, 120, 90),\n",
        "    (153, 153, 153), (250, 170, 30), (220, 220, 0), (107, 142, 35),\n",
        "    (152, 251, 152), (70, 130, 180), (220, 20, 60), (255, 0, 0),\n",
        "    (0, 0, 142), (0, 0, 70), (0, 60, 100), (0, 0, 90), (0, 0, 110),\n",
        "    (0, 80, 100), (0, 0, 230), (119, 11, 32), (0, 0, 142)\n",
        "], dtype=np.uint8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb2913e9",
      "metadata": {
        "id": "bb2913e9"
      },
      "source": [
        "- Set IMG_HEIGHT, IMG_WIDTH respectively to 96 and 256\n",
        "- Set NUM_CLASSES to 30\n",
        "- Set the batch size, BATCH_SIZE to 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9c8062e9",
      "metadata": {
        "id": "9c8062e9"
      },
      "outputs": [],
      "source": [
        "# Write your code in this cell\n",
        "IMG_HEIGHT, IMG_WIDTH = 96, 256\n",
        "NUM_CLASSES = 30\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5790dc02",
      "metadata": {
        "id": "5790dc02"
      },
      "outputs": [],
      "source": [
        "# ========== GDN LAYER ==========\n",
        "class NonNegConstraint:\n",
        "    def __call__(self, tensor):\n",
        "        return torch.clamp(tensor, min=1e-15)\n",
        "\n",
        "class GDN(nn.Module):\n",
        "    def __init__(self, in_channels, filter_size=3):\n",
        "        super(GDN, self).__init__()\n",
        "        self.filter_size = filter_size\n",
        "        self.padding = (filter_size - 1) // 2\n",
        "\n",
        "        # Parameters\n",
        "        self.beta = nn.Parameter(torch.ones(in_channels))\n",
        "        self.alpha = nn.Parameter(torch.ones(in_channels), requires_grad=False)\n",
        "        self.epsilon = nn.Parameter(torch.ones(in_channels), requires_grad=False)\n",
        "\n",
        "        # Gamma weights (constrained to be non-negative)\n",
        "        self.gamma = nn.Parameter(torch.zeros(filter_size, filter_size, in_channels, in_channels))\n",
        "\n",
        "        # Apply constraints\n",
        "        self.constraint = NonNegConstraint()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply constraints\n",
        "        with torch.no_grad():\n",
        "            self.beta.data = self.constraint(self.beta.data)\n",
        "            self.alpha.data = self.constraint(self.alpha.data)\n",
        "            self.epsilon.data = self.constraint(self.epsilon.data)\n",
        "            self.gamma.data = self.constraint(self.gamma.data)\n",
        "\n",
        "        # Compute normalization\n",
        "        abs_x = torch.abs(x)\n",
        "        norm_conv = F.conv2d(\n",
        "            abs_x ** self.alpha.view(1, -1, 1, 1),\n",
        "            self.gamma.permute(3, 2, 0, 1),  # [out_c, in_c, h, w]\n",
        "            padding=self.padding,\n",
        "            groups=self.gamma.shape[3]  # Depthwise convolution\n",
        "        )\n",
        "\n",
        "        norm = self.beta.view(1, -1, 1, 1) + norm_conv\n",
        "        norm = norm ** self.epsilon.view(1, -1, 1, 1)\n",
        "\n",
        "        return x / norm\n",
        "\n",
        "# ========== DATASET ==========\n",
        "class CityscapesDataset(Dataset):\n",
        "    def __init__(self, image_folder, mask_folder, image_names, mask_names,\n",
        "                 height=IMG_HEIGHT, width=IMG_WIDTH, transform=None, is_train=True):\n",
        "        self.image_folder = image_folder\n",
        "        self.mask_folder = mask_folder\n",
        "        self.image_names = image_names\n",
        "        self.mask_names = mask_names\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.transform = transform\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_path = os.path.join(self.image_folder, self.image_names[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Load mask\n",
        "        mask_path = os.path.join(self.mask_folder, self.mask_names[idx])\n",
        "        mask = Image.open(mask_path).convert('RGB')\n",
        "\n",
        "        # Resize\n",
        "        image = image.resize((self.width, self.height), Image.BILINEAR)\n",
        "        mask = mask.resize((self.width, self.height), Image.NEAREST)\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        image = np.array(image).astype(np.float32) / 255.0\n",
        "        mask = np.array(mask).astype(np.int32)\n",
        "\n",
        "        # One-hot encode mask\n",
        "        one_hot_mask = np.zeros((self.height, self.width, NUM_CLASSES), dtype=np.float32)\n",
        "        for i, color in enumerate(colors):\n",
        "            class_map = np.all(mask == color, axis=-1)\n",
        "            one_hot_mask[:, :, i] = class_map\n",
        "\n",
        "        # Convert to tensors\n",
        "        image = torch.from_numpy(image).permute(2, 0, 1)  # [C, H, W]\n",
        "        one_hot_mask = torch.from_numpy(one_hot_mask).permute(2, 0, 1)  # [C, H, W]\n",
        "\n",
        "        return image, one_hot_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "cf2b5571",
      "metadata": {
        "id": "cf2b5571"
      },
      "outputs": [],
      "source": [
        "# ========== MODEL ==========\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, pool=True, dropout=0.2):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.pool = pool\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.dropout1 = nn.Dropout2d(dropout)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.dropout2 = nn.Dropout2d(dropout)\n",
        "\n",
        "        if pool:\n",
        "            self.pool_layer = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        if self.pool:\n",
        "            return x, self.pool_layer(x)\n",
        "        return x\n",
        "\n",
        "class UNetGDN(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=NUM_CLASSES):\n",
        "        super(UNetGDN, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.gdn = GDN(in_channels)\n",
        "        self.enc1 = ConvBlock(in_channels, 16, pool=True)\n",
        "        self.enc2 = ConvBlock(16, 32, pool=True)\n",
        "        self.enc3 = ConvBlock(32, 64, pool=True)\n",
        "        self.enc4 = ConvBlock(64, 128, pool=True)\n",
        "\n",
        "        # Bridge\n",
        "        self.bridge = ConvBlock(128, 256, pool=False)\n",
        "\n",
        "        # Decoder\n",
        "        self.up1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec1 = ConvBlock(256, 128, pool=False)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec2 = ConvBlock(128, 64, pool=False)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
        "        self.dec3 = ConvBlock(64, 32, pool=False)\n",
        "\n",
        "        self.up4 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
        "        self.dec4 = ConvBlock(32, 16, pool=False)\n",
        "\n",
        "        # Output\n",
        "        self.out_conv = nn.Conv2d(16, num_classes, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        g1 = self.gdn(x)\n",
        "        x1, p1 = self.enc1(g1)\n",
        "        x2, p2 = self.enc2(p1)\n",
        "        x3, p3 = self.enc3(p2)\n",
        "        x4, p4 = self.enc4(p3)\n",
        "\n",
        "        # Bridge\n",
        "        b1 = self.bridge(p4)\n",
        "\n",
        "        # Decoder\n",
        "        u1 = self.up1(b1)\n",
        "        c1 = torch.cat([u1, x4], dim=1)\n",
        "        x5 = self.dec1(c1)\n",
        "\n",
        "        u2 = self.up2(x5)\n",
        "        c2 = torch.cat([u2, x3], dim=1)\n",
        "        x6 = self.dec2(c2)\n",
        "\n",
        "        u3 = self.up3(x6)\n",
        "        c3 = torch.cat([u3, x2], dim=1)\n",
        "        x7 = self.dec3(c3)\n",
        "\n",
        "        u4 = self.up4(x7)\n",
        "        c4 = torch.cat([u4, x1], dim=1)\n",
        "        x8 = self.dec4(c4)\n",
        "\n",
        "        # Output\n",
        "        out = self.out_conv(x8)\n",
        "        return self.softmax(out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4e9b5dc1",
      "metadata": {
        "id": "4e9b5dc1"
      },
      "outputs": [],
      "source": [
        "# ========== METRICS ==========\n",
        "def iou_metrics(y_true, y_pred, num_classes=NUM_CLASSES):\n",
        "    \"\"\"Calculate mean IoU\"\"\"\n",
        "    ious = []\n",
        "    batch_size = y_true.shape[0]\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Flatten predictions and ground truth\n",
        "        pred_flat = y_pred[i].reshape(-1).cpu().numpy()\n",
        "        true_flat = torch.argmax(y_true[i], dim=0).reshape(-1).cpu().numpy()\n",
        "\n",
        "        # Calculate IoU for each class\n",
        "        iou = jaccard_score(true_flat, pred_flat,\n",
        "                           average='macro',\n",
        "                           labels=np.arange(num_classes),\n",
        "                           zero_division=0)\n",
        "        ious.append(iou)\n",
        "\n",
        "    return np.mean(ious)\n",
        "\n",
        "def color_to_one_hot_mask(mask, colors, height=IMG_HEIGHT, width=IMG_WIDTH):\n",
        "    \"\"\"Convert class indices to colored mask\"\"\"\n",
        "    color_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "    mask_np = mask.cpu().numpy() if torch.is_tensor(mask) else mask\n",
        "\n",
        "    for c in range(len(colors)):\n",
        "        color_true = mask_np == c\n",
        "        for i in range(3):\n",
        "            color_mask[:, :, i] += color_true * colors[c][i]\n",
        "\n",
        "    return color_mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d0c0b82f",
      "metadata": {
        "id": "d0c0b82f"
      },
      "outputs": [],
      "source": [
        "# ========== TRAINING FUNCTION ==========\n",
        "def train_model(seed, save_dir):\n",
        "    print(f'\\n{\"=\"*50}')\n",
        "    print(f'STARTING TRAINING WITH SEED {seed}')\n",
        "    print(f'{\"=\"*50}')\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Create save directory\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Get file lists\n",
        "    train_images_names = sorted([f for f in os.listdir(train_images_folder_path) if f.endswith('.png')])\n",
        "    train_mask_names = sorted([f for f in os.listdir(train_mask_folder_path) if f.endswith('.png')])\n",
        "    test_images_names = sorted([f for f in os.listdir(test_images_folder_path) if f.endswith('.png')])\n",
        "    test_mask_names = sorted([f for f in os.listdir(test_mask_folder_path) if f.endswith('.png')])\n",
        "\n",
        "    # Split into train/val (300 for validation)\n",
        "    total_train = len(train_images_names)\n",
        "    val_indices = random.sample(range(total_train), 300)\n",
        "    train_indices = [i for i in range(total_train) if i not in val_indices]\n",
        "\n",
        "    train_img_names = [train_images_names[i] for i in train_indices]\n",
        "    train_msk_names = [train_mask_names[i] for i in train_indices]\n",
        "    val_img_names = [train_images_names[i] for i in val_indices]\n",
        "    val_msk_names = [train_mask_names[i] for i in val_indices]\n",
        "\n",
        "    print(f'Train samples: {len(train_img_names)}')\n",
        "    print(f'Val samples: {len(val_img_names)}')\n",
        "    print(f'Test samples: {len(test_images_names)}')\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = CityscapesDataset(\n",
        "        train_images_folder_path, train_mask_folder_path,\n",
        "        train_img_names, train_msk_names\n",
        "    )\n",
        "\n",
        "    val_dataset = CityscapesDataset(\n",
        "        train_images_folder_path, train_mask_folder_path,\n",
        "        val_img_names, val_msk_names\n",
        "    )\n",
        "\n",
        "    test_dataset = CityscapesDataset(\n",
        "        test_images_folder_path, test_mask_folder_path,\n",
        "        test_images_names, test_mask_names, is_train=False\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Initialize model\n",
        "    model = UNetGDN(in_channels=3, num_classes=NUM_CLASSES).to(device)\n",
        "    print(f'\\nModel architecture:')\n",
        "    print(model)\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.L1Loss()  # MAE loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=15, verbose=True)\n",
        "\n",
        "    # Training variables\n",
        "    best_iou = 0.0\n",
        "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'val_iou': []}\n",
        "\n",
        "    # Training loop\n",
        "    epochs = 300\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')\n",
        "        for images, masks in pbar:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            true_labels = torch.argmax(masks, dim=1)\n",
        "            correct = (preds == true_labels).sum().item()\n",
        "\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "            train_correct += correct\n",
        "            train_total += true_labels.numel()\n",
        "\n",
        "            pbar.set_postfix({'Loss': loss.item(), 'Acc': correct/true_labels.numel()})\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        all_preds = []\n",
        "        all_masks = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, masks in val_loader:\n",
        "                images, masks = images.to(device), masks.to(device)\n",
        "                outputs = model(images)\n",
        "\n",
        "                loss = criterion(outputs, masks)\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                true_labels = torch.argmax(masks, dim=1)\n",
        "                correct = (preds == true_labels).sum().item()\n",
        "\n",
        "                val_correct += correct\n",
        "                val_total += true_labels.numel()\n",
        "\n",
        "                all_preds.append(preds.cpu())\n",
        "                all_masks.append(masks.cpu())\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        # Calculate IoU\n",
        "        all_preds = torch.cat(all_preds, dim=0)\n",
        "        all_masks = torch.cat(all_masks, dim=0)\n",
        "        val_iou = iou_metrics(all_masks, all_preds)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(val_iou)\n",
        "\n",
        "        # Save best model\n",
        "        if val_iou > best_iou:\n",
        "            best_iou = val_iou\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_iou': val_iou,\n",
        "                'val_loss': val_loss,\n",
        "            }, os.path.join(save_dir, 'best_model.pth'))\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['val_iou'].append(val_iou)\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
        "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val IoU: {val_iou:.4f}')\n",
        "\n",
        "        # Save checkpoint every 50 epochs\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'history': history,\n",
        "            }, os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
        "\n",
        "    # Load best model for testing\n",
        "    checkpoint = torch.load(os.path.join(save_dir, 'best_model.pth'))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Test phase\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    test_preds = []\n",
        "    test_masks = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in test_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            true_labels = torch.argmax(masks, dim=1)\n",
        "            correct = (preds == true_labels).sum().item()\n",
        "\n",
        "            test_correct += correct\n",
        "            test_total += true_labels.numel()\n",
        "\n",
        "            test_preds.append(preds.cpu())\n",
        "            test_masks.append(masks.cpu())\n",
        "\n",
        "    test_acc = test_correct / test_total\n",
        "    test_preds = torch.cat(test_preds, dim=0)\n",
        "    test_masks = torch.cat(test_masks, dim=0)\n",
        "    test_iou = iou_metrics(test_masks, test_preds)\n",
        "\n",
        "    print(f'\\nTest Results:')\n",
        "    print(f'Test Accuracy: {test_acc:.4f}')\n",
        "    print(f'Test IoU: {test_iou:.4f}')\n",
        "    print(f'Best Validation IoU: {best_iou:.4f}')\n",
        "\n",
        "    # Save history\n",
        "    np.save(os.path.join(save_dir, 'history.npy'), history)\n",
        "\n",
        "    # Plot training curves\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    axes[0, 0].plot(history['train_loss'], label='Train')\n",
        "    axes[0, 0].plot(history['val_loss'], label='Validation')\n",
        "    axes[0, 0].set_title('Model Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "\n",
        "    axes[0, 1].plot(history['train_acc'], label='Train')\n",
        "    axes[0, 1].plot(history['val_acc'], label='Validation')\n",
        "    axes[0, 1].set_title('Model Accuracy')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "\n",
        "    axes[1, 0].plot(history['val_iou'])\n",
        "    axes[1, 0].set_title('Validation IoU')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('IoU')\n",
        "    axes[1, 0].grid(True)\n",
        "\n",
        "    # Visualize predictions\n",
        "    axes[1, 1].axis('off')\n",
        "    axes[1, 1].text(0.5, 0.5, f'Best Val IoU: {best_iou:.4f}\\nTest IoU: {test_iou:.4f}\\nTest Acc: {test_acc:.4f}',\n",
        "                   ha='center', va='center', fontsize=12, bbox=dict(boxstyle=\"round\", fc=\"w\"))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'training_curves.png'), dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Generate prediction visualization\n",
        "    visualize_predictions(model, test_dataset, colors, save_dir)\n",
        "\n",
        "    return model, history, test_iou, test_acc\n",
        "\n",
        "def visualize_predictions(model, dataset, colors, save_dir, num_samples=5):\n",
        "    \"\"\"Visualize model predictions\"\"\"\n",
        "    model.eval()\n",
        "    indices = random.sample(range(len(dataset)), num_samples)\n",
        "\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples*4))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        image, mask = dataset[idx]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(image.unsqueeze(0).to(device))\n",
        "            pred = torch.argmax(output, dim=1).squeeze(0).cpu()\n",
        "\n",
        "        # Original image\n",
        "        axes[i, 0].imshow(image.permute(1, 2, 0).numpy())\n",
        "        axes[i, 0].set_title('Original Image')\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Ground truth mask\n",
        "        true_mask = torch.argmax(mask, dim=0)\n",
        "        colored_true = color_to_one_hot_mask(true_mask, colors)\n",
        "        axes[i, 1].imshow(colored_true)\n",
        "        axes[i, 1].set_title('Ground Truth')\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        # Predicted mask\n",
        "        colored_pred = color_to_one_hot_mask(pred, colors)\n",
        "        axes[i, 2].imshow(colored_pred)\n",
        "        axes[i, 2].set_title('Prediction')\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'predictions.png'), dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26979504",
      "metadata": {
        "id": "26979504"
      },
      "source": [
        "Main execution code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "eabaf6bb",
      "metadata": {
        "id": "eabaf6bb",
        "outputId": "052515df-631b-4578-9e32-dd12a250c866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "STARTING TRAINING WITH SEED 0\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/cityscapes/train/img'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3900681474.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msave_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'./Good_train/1_gdn/Train_{i}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iou\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         results.append({\n",
            "\u001b[0;32m/tmp/ipython-input-1367735421.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(seed, save_dir)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Get file lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_images_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images_folder_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_mask_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mask_folder_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtest_images_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images_folder_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/cityscapes/train/img'"
          ]
        }
      ],
      "source": [
        "# ========== MAIN EXECUTION ==========\n",
        "if __name__ == \"__main__\":\n",
        "    seeds = [0]  # You can add more seeds: [0, 11, 25, 333, 41, 55, 666, 70, 8, 123]\n",
        "\n",
        "    results = []\n",
        "    for i, seed in enumerate(seeds):\n",
        "        save_dir = f'./Good_train/1_gdn/Train_{i}'\n",
        "        model, history, test_iou, test_acc = train_model(seed, save_dir)\n",
        "\n",
        "        results.append({\n",
        "            'seed': seed,\n",
        "            'test_iou': test_iou,\n",
        "            'test_acc': test_acc,\n",
        "            'best_val_iou': max(history['val_iou'])\n",
        "        })\n",
        "\n",
        "    # Print summary\n",
        "    print('\\n' + '='*50)\n",
        "    print('TRAINING SUMMARY')\n",
        "    print('='*50)\n",
        "    for res in results:\n",
        "        print(f\"Seed {res['seed']}: Test IoU: {res['test_iou']:.4f}, \"\n",
        "              f\"Test Acc: {res['test_acc']:.4f}, Best Val IoU: {res['best_val_iou']:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}